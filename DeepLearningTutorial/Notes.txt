https://www.kaggle.com/learn/deep-learning


Intro to DL for Computer Vision
-Course uses Tensorflow and Keras (Python, obv)
-Course will start at high level instead of theory
-tensors, blah blah blah
-a convolution is a small tensor that can be multiplied over a section of the image tensor - also called filters
-each individual convolution can be used to identify/detect specific features in an image

Building Models From Convolutions
-model training determines the convolutions
-this is performed with gradient descent, backpropagation, etc.
-when running an image through a convolution, it creates a 2D tensor
-the 2D tensors from all convolutions are stacked into a 3D tensor
-moving through the different 2D tensors in the 3D tensor (the 2D tensors from the convolutions) is called the "channel" dimension
-we apply another layer of convolutions to the 3D tensor (a second layer in the CNN), etc., etc.
-once we finish with layers, we are left over with a 3D tensor that describes the image contents

Transfer Learning
-we can take a pre-trained image recognition network model and modify it slightly to identify new categories
-the higher the level of the network layer, the more complex the shape (convolution) it identifies
-knowing this, we can take a pre-trained CNN and remove the top (prediction) layer
-we can replace this with our new categories in order to take advantage of the existing convolution training

Data Augmentation
-we can create more training images by flipping or cropping/shifting our training set (can probably rotate, etc. as well)
-use caution - this should not always be used
-judge on a case-by-case basis to determine if the modified image should be in the same category as the original
-a "dense" layer in a NN is just a layer where all the nodes in the layer are connected to all the nodes in the previous layer
-likewise, a dense NN is an entire network made of dense layers
-an "epoch" is when a dataset is processed through the entire NN once; mulitple epochs are used to improve model accuracy and reduce over/underfitting

A Deeper Understanding of Deep Learning
-forward prop, each connection between nodes has a weight
-to find the value of a node: sum the product of the value of each imput node and the weight of the node connection to get the current node's value
-hidden layers are non-input or prediction layers in the network
-stochasticor mini-batch gradient descent (not batch) tends to work better for NNs because of the computing power required to run all examples at once

Dropout and Strides for Larger Models
-dropout is a way to remove random nodes from calculation during training
-the nodes are not removed for all of training, but only for part of it
-this helps to reduce the likelihood of overfitting
-strides are the number of pixels to move per step for sliding windows (default is one)
-increasing the stride speeds up training and predictions
